---
title: "P8105_hw3_ys3394"
author: "Yifan Su"
date: "10/10/2020"
output: github_document
---

```{r setup, include=FALSE}
# Load necessary packages
library(tidyverse)
library(readxl)
library(patchwork) 
library(hexbin)
library(p8105.datasets)

# set options
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

#### Do some exploration of the dataset "instacart".

```{r instacart}
data("instacart")
```

**A Short description**

* This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. 

* This instacart online grocery shopping dataset has information about orders of users on the instacart, with each row represents the data of one product of an order. 

* Key variables of user and order are -- user ID, order day and order hour. They are other variables about items and products, including aisle, department and their ID numbers. It's worth noticing that there are many aisles in a department, and products with different aisles and departments ID are distinct.


**(1) Number of ailes, and show aisles that most items ordered from**

```{r aisles, collapse=TRUE}
# count aisles in instacart, and rank them
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

* They are `r instacart %>% distinct(aisle) %>% count()` ailes in this dataset, and the item most ordered from is the `fresh vegetables`.


**(2) Make a plot shows number of items and those greater than 10000 items ordered**

```{r items}
# tidy data, and plot
instacart %>% 
  count(aisle) %>%
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%  # convert aisle to a factor variable, then we can use the function fct_reorder to adjust order.
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Aisles with more than 10000 items ordered",
    x = "Aisles' names",
    y = "Number of orders",
    caption = "Data from instacart online grocery shopping dataset 2017"
  )
```

* The plot above shows number of items greater than 10000, with item `fresh vegetables` and `fresh fruits` being the top two.


**(3) Make a table showing the three most popular items in each of the aisles**

```{r three_items}
# list three items that are most popular among aisles
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(
    rank = min_rank(desc(n))
  ) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable(digits = 2) # make a table
```

* The table ablve shows top three popular items in aisles `baking ingredients`, `dog food care` and `packaged vegetables fruits`, respectively.


**(4) Make a table of the mean hour of the day of two items in a week**

```{r mean_hour, warning=FALSE, message=FALSE}
# a plot about mean hour of the day
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow, .groups = 'drop') %>% # drop group to eliminate warning message
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider( # increase readability
    names_from = order_dow,
    values_from = mean_hour,
    names_prefix = "mean_order_hour_of_day_"
  ) %>% 
  t() %>% 
  knitr::kable(digits = 2)
```

* The table above showsthe mean hour of the day at which `Pink Lady Apples` and `Coffee Ice Cream` are ordered on each day of the week. Mean hour information regarding these two items are quite similar.


## Problem 2

**(1) Load, tidy, and otherwise wrangle the data**

```{r accelerometers, warning=FALSE, collapse=TRUE}
# Load data accel_data.csv
chf_df =
  read_csv("./data/accel_data.csv", col_types = cols()) %>% # col_types also remove warnings
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_day",
    values_to = "activity_counts",
    names_prefix = "activity_"
    ) %>% 
  mutate(
    minute_day = as.integer(minute_day),
    week = as.factor(week),
    day_id = as.factor(day_id),
    day = as.factor(day),
    weekday_weekend = case_when( 
      day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "weekday",
      day %in% c("Saturday", "Sunday") ~ "weekend"),
    weekday_weekend = as.factor(weekday_weekend),
    day = forcats::fct_relevel(day, "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
    ) # add a weekday_weekend variable

head(chf_df)
```

In this dataset, there are `r nrow(chf_df)` rows and `r ncol(chf_df)` columns. Variables in it are `r names(chf_df)`. Among which, variable `minute_day` and `activity_counts` corresponding to each minute over a day and activity counts, respectively. Also, a variable `weekday_weekend` is added to distinguish weekday from weekend.


**(2) Traditional analyses of accelerometer data focus on the total activity over the day**

```{r activity_day, warning=FALSE}
# total activity over the day
chf_df_2 =
  chf_df %>% 
  group_by(week, day, .groups = 'drop') %>% 
  summarize(sum_counts = sum(activity_counts)) %>% # count total activity counts
  pivot_wider(
    names_from = day,
    values_from = sum_counts
  ) %>% 
  knitr::kable(digits = 2)

chf_df_2
```

* This table shows total activity of the male over days in five weeks. There is a trend that activity counts in week 2 are larger than that of other weeks, while it has the least counts overall for week 4. On Saturday of week 3 and week 4, the counts are low.


**(3) Accelerometer data allows the inspection activity over the course of the day**

```{r plot, warning=FALSE, message=FALSE, collapse=TRUE}
# activity over the course of the day
chf_df %>% 
  ggplot(aes(x = minute_day, y = activity_counts, color = day)) +
  geom_smooth(se = FALSE) +
  labs( 
    title = "Inspection activity over day",
    x = "Hours",
    y = "Activity counts",
    caption = "Data from instacart online grocery shopping dataset 2017"
  ) +
   viridis::scale_color_viridis( # a color package
    name = "day",
    discrete =  TRUE
   ) +
  scale_y_continuous(trans = "sqrt") + # transform the plot, more readable
  scale_x_continuous(breaks = c(1, 240, 480, 720, 960, 1200, 1440), 
                     labels = c("0h","4h", "8h", "12h", "16h", "20h", "24h")) +
  theme_minimal() 
```

* This graph shows the activity counts from Monday to Sunday. It shows that there are apparently two peaks, one for Friday at 22h and one for Sunday at 11h, have high activity counts. Also, there is the trend that high activity counts are gained from middle of the day, or at night around 21h. Besides, the 4th hour of a day usually has the lowest value for activity counts.


## Problem 3

#### Do some exploration of the dataset "NOAA".

```{r load_nynoaa}
library(p8105.datasets)
data("ny_noaa")
```

**Short description**

* The dataset "NY NOAA" is the weather data provided by the NOAA, including summary statistics from weather stations. It is a dataframe contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. 

* Key variables of it are **_`r names(ny_noaa)`_**. Among them, there are variable `id` showing information of weather stations, and summary weather data such as `prcp`, `tmax` and `tmin` showing precipitation and temperature conditions. 

* For variables `prcp`, `snow` and `snwd`, the proportions of missing value are relatively low. In variables `tmax` and `tmin`, missing values consist of nearly half of their total values, which are extreamly high. More detailed information about variable missing values can be seen in the table below.

```{r missing_value, echo=FALSE, message=FALSE}
# count missing values for each columns
colMeans(is.na(ny_noaa)) %>% 
  knitr::kable()
```


**(1) Data cleaning**

```{r ny_noaa_df}
# tidy ny_noaa dataset
ny_noaa_df =
  ny_noaa %>% 
  mutate_at(vars(date), as.factor) %>% # mutate data prior to separate, data in the wrong class
  separate(date, into = c("year", "month", "day"), "-") %>% 
  mutate_at(vars(prcp, tmax, tmin, snow), as.numeric) %>% 
  mutate(
    prcp = prcp/10, # these three variables are in tenth
    tmin = tmin/10,
    tmax = tmax/10,
    snow = case_when( # remove snow values < 0 
    snow < 0 ~ 0,
    snow >= 0 ~ snow)
  )
```

```{r snow, collapse=TRUE}
# find the most common snowfall value
ny_noaa_df %>% 
  count(snow, na.rm = TRUE) %>% 
  mutate(snow_rank = min_rank(desc(n))) %>% 
  filter(snow_rank == "1")
```

* The most common observed value for snowfall is 0. Since the data is tidied, unreasonable snowfall values less than 0 has been screened, so the result is reliable. It makes sense that the most common value of snowfall is O that there is no snow for most time of the year.


**(2) Make a two-panel plot showing the average max temperature in January and in July in each station across years**

```{r two_panel, warning=FALSE, message=FALSE, collapse=TRUE}
# average max temperature in January
month_01 =
ny_noaa_df %>% 
  filter(month == "01") %>% 
  group_by(id, year, month) %>% 
  summarize(tmax_mean = mean(tmax, na.rm = TRUE), .groups = 'drop') %>% 
  drop_na(tmax_mean) %>% 
  ggplot(aes(x = year, y = tmax_mean, color = id)) +
  geom_point(alpha = 0.5, size = 0.1) +
  geom_path(aes(group = id), alpha = 0.3, size = 0.2) + # connect points with lines
  theme_minimal() +
  theme(
    legend.position = 'none',
    axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1),
    ) +
  labs(
    x = "Year",
    y = "Temperature(C)",
    title = "Max temperature in January per year",
    caption = "Data from NY NOAA"
    )

# average max temperature in July
month_07 =
ny_noaa_df %>% 
  filter(month == "07") %>% 
  group_by(id, year, month, .groups = 'drop') %>% 
  summarize(tmax_mean = mean(tmax, na.rm = TRUE)) %>% 
  drop_na(tmax_mean) %>% 
  ggplot(aes(x = year, y = tmax_mean, color = id)) +
  geom_point(alpha = 0.5, size = 0.1) +
  geom_path(aes(group = id), alpha = 0.3, size = 0.2) +
  theme_minimal() +
  theme(
    legend.position = 'none',
    axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1),
    ) +
  labs(
    x = "Year",
    y = "Temperature(C)",
    title = "Max temperature in July per year",
    caption = "Data from NY NOAA"
    )  

# combine two panels
month_01 / month_07
```

__Outliers of temperature in January and July__
```{r outliers, collapse=TRUE, warning=FALSE, message=FALSE}
# Outliers of temperature in January accrose the years
outlier_01 =
  ny_noaa_df %>% 
  filter(month == "01") %>% 
  group_by(id, year, month, .groups = 'drop') %>% 
  summarize(tmax_mean = mean(tmax, na.rm = TRUE)) %>% 
  drop_na(tmax_mean) %>% 
  filter(tmax_mean > 10 | tmax_mean < -10) %>% 
  count()

outlier_01
  
# Outliers of temperature in July accrose the years
outlier_07 =
  ny_noaa_df %>% 
  filter(month == "07") %>% 
  group_by(id, year, month, .groups = 'drop') %>% 
  summarize(tmax_mean = mean(tmax, na.rm = TRUE)) %>% 
  drop_na(tmax_mean) %>% 
  filter(tmax_mean > 33 | tmax_mean < 20) %>% 
  count()

outlier_07
```

* For maximum temperature in January over years, there are two years has apparently low temperature, which are year 1994 and year 2004. Although temperature in January fluctuated over years, temperature observed in different weather stations are paralled over years.

* For maximum temperature in July over years, values are less regular than that of January, while it doesn't fluctuate more than that of January. The overall maximum temperature of July is higher than in January. Also, some outliers are recorded in year 1988, year 2004, and year 2007.


**(3) Make a two-panel plot showing tmin vs tmax, and snowfall distribution**

```{r temp_snow, message=FALSE, warning=FALSE}
# plot for tmax_vs_tmin over years
tmax_vs_tmin =
ny_noaa_df %>% 
  drop_na(tmax, tmin) %>% 
  pivot_longer(
    tmax:tmin,
    names_to = "tmax_tmin", # combine tmax and tmin to one variable
    values_to = "temperature"
  ) %>% 
  ggplot(aes(x = year, y = temperature, color = tmax_tmin)) +
  geom_hex(alpha = .2) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1)) +
  labs(
    x = "Year",
    y = "Temperature(C)",
    title = "Maximun and minimum temperature across years",
    caption = "Data from NY NOAA"
    )

# plot for snowfall distribution over years
snowfall =
ny_noaa_df %>% 
  drop_na(snow) %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_violin(alpha = .2, color = "blue", fill = "blue") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1)) +
  labs(
    x = "Year",
    y = "Snowfall(mm)",
    title = "Snallfall distribution over years",
    caption = "Data from NY NOAA"
    )  

# combine two panels
(tmax_vs_tmin / snowfall) + plot_layout(widths = 8, heights = 16)
```

* For the plot showing maximum and minimum temperature over year, the changing of maximum temperature are paralleled with the minimum temperature over years. And it's for sure that the maximum temperature are higher than the minimum temperature of a year. Also, the change of maximum temperature value seems to lag the change of minumum temperature value.

* For the plot showing snowfall distribution, the common snowfall value over years are 50 and 75 (mm), and some of those values fall between 0 to 25 (mm). And there is a trend that the amount of snowfall is decreasing over years.

